{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5e85a2",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7362da",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec26df",
   "metadata": {},
   "source": [
    "Here, we will explore the PyTorch package. To download PyTorch, visit [this link](https://pytorch.org/get-started/locally/). Tutorials can be found [here](https://pytorch.org/tutorials). We will start below with the [Introduction to Pytorch tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html).\n",
    "\n",
    "It is highly recommended to, after the introduction to PyTorch tutorial, look at different [PyTorch recipes](https://pytorch.org/tutorials/recipes/recipes_index.html). These are small, bite-sized tutorials of specific features, and so at the same time propose different ways to extend the general, basic PyTorch tutorial shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89133be1",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18265bcf",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68795f",
   "metadata": {},
   "source": [
    "This quickstart deals with all the features that are going to be explained in more detail in separate sections below. Here, we provide a quick overview of most common way to start using neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e722f",
   "metadata": {},
   "source": [
    "### Working with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea28326",
   "metadata": {},
   "source": [
    "PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e934668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibeauwouters/miniconda3/envs/igwn-py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/thibeauwouters/miniconda3/envs/igwn-py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02238490",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset. The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the FashionMNIST dataset (more information about this dataset can be found [on this Kaggle page](https://www.kaggle.com/datasets/zalando-research/fashionmnist)). Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab18fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4785771",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ac1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3aaff",
   "metadata": {},
   "source": [
    "### Creating models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d7882",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network in the __ init __ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31c64ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e22b3a",
   "metadata": {},
   "source": [
    "### Optimizing model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f2a11",
   "metadata": {},
   "source": [
    "To train a model, we need a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e261c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739df9f",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f76219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d691d",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fdc977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425915a",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf01def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296108  [    0/60000]\n",
      "loss: 2.284835  [ 6400/60000]\n",
      "loss: 2.265504  [12800/60000]\n",
      "loss: 2.263885  [19200/60000]\n",
      "loss: 2.234288  [25600/60000]\n",
      "loss: 2.216710  [32000/60000]\n",
      "loss: 2.219333  [38400/60000]\n",
      "loss: 2.177929  [44800/60000]\n",
      "loss: 2.178759  [51200/60000]\n",
      "loss: 2.159082  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 2.142435 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.151727  [    0/60000]\n",
      "loss: 2.140112  [ 6400/60000]\n",
      "loss: 2.075284  [12800/60000]\n",
      "loss: 2.091818  [19200/60000]\n",
      "loss: 2.030401  [25600/60000]\n",
      "loss: 1.978954  [32000/60000]\n",
      "loss: 2.004168  [38400/60000]\n",
      "loss: 1.914210  [44800/60000]\n",
      "loss: 1.928390  [51200/60000]\n",
      "loss: 1.862888  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.853600 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.890799  [    0/60000]\n",
      "loss: 1.857612  [ 6400/60000]\n",
      "loss: 1.729984  [12800/60000]\n",
      "loss: 1.771081  [19200/60000]\n",
      "loss: 1.665290  [25600/60000]\n",
      "loss: 1.624147  [32000/60000]\n",
      "loss: 1.644337  [38400/60000]\n",
      "loss: 1.542809  [44800/60000]\n",
      "loss: 1.574693  [51200/60000]\n",
      "loss: 1.480594  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.494703 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.563581  [    0/60000]\n",
      "loss: 1.531411  [ 6400/60000]\n",
      "loss: 1.374691  [12800/60000]\n",
      "loss: 1.446646  [19200/60000]\n",
      "loss: 1.333840  [25600/60000]\n",
      "loss: 1.336431  [32000/60000]\n",
      "loss: 1.349041  [38400/60000]\n",
      "loss: 1.273504  [44800/60000]\n",
      "loss: 1.306109  [51200/60000]\n",
      "loss: 1.223317  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.243929 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.319346  [    0/60000]\n",
      "loss: 1.306445  [ 6400/60000]\n",
      "loss: 1.133450  [12800/60000]\n",
      "loss: 1.238493  [19200/60000]\n",
      "loss: 1.113630  [25600/60000]\n",
      "loss: 1.146955  [32000/60000]\n",
      "loss: 1.165756  [38400/60000]\n",
      "loss: 1.104771  [44800/60000]\n",
      "loss: 1.136464  [51200/60000]\n",
      "loss: 1.069291  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.086258 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e46ca",
   "metadata": {},
   "source": [
    "### Save a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a7191",
   "metadata": {},
   "source": [
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d97a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b8bba",
   "metadata": {},
   "source": [
    "### Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684ab4a",
   "metadata": {},
   "source": [
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4866f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a3130",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da54ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf11f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402080af",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f4e81",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebafaba",
   "metadata": {},
   "source": [
    "### Initializing a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dc70d",
   "metadata": {},
   "source": [
    "Tensors can be initialized: \n",
    "1. directly from data\n",
    "2. from a Numpy array\n",
    "3. from another tensor\n",
    "4. using `shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8927d4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.4150, 0.7056],\n",
      "        [0.8643, 0.5718]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.4292, 0.9468, 0.3598],\n",
      "        [0.6412, 0.6343, 0.0717]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "#2\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "#3\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "#4\n",
    "shape = (2,3,) # NOTE - #rows = 2, #columns = 3\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e2a64",
   "metadata": {},
   "source": [
    "### Attributes of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a6882f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      " \n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones((3, 4))\n",
    "\n",
    "print(f\"Tensor: \\n {tensor} \\n \")\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e63779",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388a32b",
   "metadata": {},
   "source": [
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e57c5f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13cd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IF present: move tensor like this:\n",
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f3d17",
   "metadata": {},
   "source": [
    "Some examples of tensor operations are provided below. Check the details of tensor operations in the tutorial [here](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors). Syntax very similar to numpy arrays.\n",
    "* first row: `tensor[0]`\n",
    "* first column: `tensor[:,0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0822fa4",
   "metadata": {},
   "source": [
    "A bit non-trivial is the `cat` method. **Note**: there is also the `stack` method, not to be confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4bac2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack along dimension = 1\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "torch.Size([3, 12])\n",
      "Stack along dimension = 0\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.Size([9, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Stack along dimension = 1\")\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "print(t1.shape)\n",
    "\n",
    "print(\"Stack along dimension = 0\")\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim = 0)\n",
    "print(t2)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67840a10",
   "metadata": {},
   "source": [
    "For arithmetic computations, there are various possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5550b337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4.],\n",
       "        [4., 4., 4.],\n",
       "        [4., 4., 4.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# note: tensors must have compatible shapes\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4219289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "# note: for this, tensors must have same shapes\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105dc109",
   "metadata": {},
   "source": [
    "Special operations exist for **one-element tensors**.  If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "324c8228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "print(agg) # agg is a one-element TENSOR\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item)) # agg_item is a regular float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca1ae8",
   "metadata": {},
   "source": [
    "Also pay attention to **in-place operations**. Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix. For example: `x.copy_(y)`, `x.t_()`, will change `x`. Note that some functions, like `add`, have an in-place operation equivalent - see the difference below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e3325e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor t: tensor([1., 1., 1., 1., 1.])\n",
      "Calling add: not stored in original tensor\n",
      "Tensor another: tensor([3., 3., 3., 3., 3.])\n",
      "Tensor t: tensor([1., 1., 1., 1., 1.])\n",
      "Calling add_: stored in original tensor\n",
      "Tensor t: tensor([3., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f'Tensor t: {t}')\n",
    "\n",
    "print(\"Calling add: not stored in original tensor\")\n",
    "another = t.add(2)\n",
    "print(f'Tensor another: {another}')\n",
    "print(f'Tensor t: {t}')\n",
    "\n",
    "print(\"Calling add_: stored in original tensor\")\n",
    "t.add_(2)\n",
    "print(f'Tensor t: {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562f64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1ddbdb0",
   "metadata": {},
   "source": [
    "### Bridge with numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfacac",
   "metadata": {},
   "source": [
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other. This is done by calling `.numpy()`. Note that, when printing, a tensor object will print `tensor()` in front of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200e8ce",
   "metadata": {},
   "source": [
    "To go from tensors to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29a220f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.]) \n",
      "\n",
      "tn [1. 1. 1. 1. 1.] \n",
      "\n",
      "Type of t: <class 'torch.Tensor'> \n",
      "\n",
      "Type of n: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "\n",
    "print(f't: {t} \\n') # note: \"tensor()\" is printed\n",
    "print(f'tn {n} \\n') # \"tensor()\" is NOT printed\n",
    "\n",
    "print(f'Type of t: {type(t)} \\n')\n",
    "print(f'Type of n: {type(n)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dd675",
   "metadata": {},
   "source": [
    "Changes are reflected into each other (note: we **have** to call `add_` for this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd39e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1) # add 1 to the tensor\n",
    "print(n) # n is changed as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277bdd7",
   "metadata": {},
   "source": [
    "From numpy array to tensor (again: changes are reflected in both objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "502f994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9591c79",
   "metadata": {},
   "source": [
    "## Datasets & dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b33b05",
   "metadata": {},
   "source": [
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find links in [this tutorial page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc6a43",
   "metadata": {},
   "source": [
    "### Loading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ecab1f",
   "metadata": {},
   "source": [
    "Here is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the FashionMNIST Dataset with the following parameters:\n",
    "* root is the path where the train/test data is stored,\n",
    "* train specifies training or test dataset,\n",
    "* download=True downloads the data from the internet if it’s not available at root.\n",
    "* transform and target_transform specify the feature and label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65dc67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-import the datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import training data. For more on arguments, see above\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Import test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc8198a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data contains 60000 items. The test data contains 60000 items.\n"
     ]
    }
   ],
   "source": [
    "print(\"The training data contains %d items. The test data contains %d items.\" % (len(training_data), len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa54119",
   "metadata": {},
   "source": [
    "### Iterating and Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a84ab",
   "metadata": {},
   "source": [
    "We can index Datasets manually like a list: `training_data[index]`. We use matplotlib to visualize some samples in our training data.\n",
    "\n",
    "Note: below we generate a random integer: we call `torch.randint()` with size 1, which creates a tensor object, and then call `item()` to get it as float, not as an item. We show these steps before showing how to use this trick to visualize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84d53887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50376])\n",
      "50376\n"
     ]
    }
   ],
   "source": [
    "random_integer_as_tensor = torch.randint(len(training_data), size=(1,))\n",
    "print(random_integer_as_tensor)\n",
    "random_integer_as_int = random_integer_as_tensor.item()\n",
    "print(random_integer_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79addd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {0: \"T-Shirt\", 1: \"Trouser\", 2: \"Pullover\",  3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n",
    "\n",
    "figure = plt.figure(figsize=(3, 3))\n",
    "cols, rows = 1, 1 # change this to plot more\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    \n",
    "    # Generate random number, get data item from it:\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    \n",
    "    # Show it:\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4375f17",
   "metadata": {},
   "source": [
    "### Creating custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db179ac5",
   "metadata": {},
   "source": [
    "A custom Dataset class must implement three functions: __ init __, __ len __, and __ getitem __. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file.\n",
    "\n",
    "In the next sections, we’ll break down what’s happening in each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a40bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd # note, we use pandas!\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        \n",
    "        # Note: img labels is a pandas dataset\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        \n",
    "        # Save other quantities\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''The __getitem__ function loads and returns a sample from the dataset at the given index idx. \n",
    "        Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, \n",
    "        retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), \n",
    "        and returns the tensor image and corresponding label in a tuple'''\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab929d0",
   "metadata": {},
   "source": [
    "### Preparing data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90f80c",
   "metadata": {},
   "source": [
    "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
    "\n",
    "DataLoader is an iterable that abstracts this complexity for us in an easy API. Below, the batch size is 64, and we shuffle it (shuffle = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e961b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "test_dataloader  = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031aab4",
   "metadata": {},
   "source": [
    "### Iterate through the DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e054c7f",
   "metadata": {},
   "source": [
    "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)).\n",
    "\n",
    "Note: `iter` creates an iterable (more info: [here](https://www.geeksforgeeks.org/python-iter-method/)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11bd4f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "\n",
    "# Plot it\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061afad6",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd69fc7",
   "metadata": {},
   "source": [
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "All TorchVision datasets have two parameters:\n",
    "1. `transform`: to modify the features, and \n",
    "2. `target_transform` to modify the labels\n",
    "\n",
    "These accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms out of the box. For the dataset, we do the following. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use ToTensor and Lambda (which is like [Python's built in lambda function](https://www.w3schools.com/python/python_lambda.asp)). \n",
    "\n",
    "1. __Modify features:__ The FashionMNIST features are in PIL Image (PIL stands for _Python Imaging Library_) format (they are the 28 x 28 pixels of each image), and the labels are integers (`0` stands for \"T-Shirt\", `1` stands for \"Trouser\", etc). Below, we call ToTensor() to deal with these features. ToTensor() converts a PIL image or NumPy ndarray into a FloatTensor and scales the image's pixel intensity values in the range [0., 1.]\n",
    "\n",
    "2. __Modify labels:__ This is done via Lambda transforms. These apply any user-defined lambda function. Here, we define a function to turn the integer (of the labels: 0 for T-shirt etc) into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y. So instead of using the integers as labels, we create 10D vectors of zeroes, and put a 1 in the index position of the initial label. The benefit of one-hot encoding tensors can be read in [this StackExchange post](https://datascience.stackexchange.com/questions/30215/what-is-one-hot-encoding-in-tensorflow)\n",
    "\n",
    "These operations are done in the final two lines of the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9c0f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value = 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42ee53",
   "metadata": {},
   "source": [
    "## Building neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be0cf8",
   "metadata": {},
   "source": [
    "Neural networks comprise of layers/modules that perform operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f937b",
   "metadata": {},
   "source": [
    "### Get the device for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1377d3b",
   "metadata": {},
   "source": [
    "We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let’s check to see if torch.cuda is available, else we continue to use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcb2cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4205f00",
   "metadata": {},
   "source": [
    "### Define the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82535150",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in __ init __. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cedae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18ea56",
   "metadata": {},
   "source": [
    "We create an instance of NeuralNetwork, and move it to the device, and print its structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "013375e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d49734",
   "metadata": {},
   "source": [
    "Note: the output is interpreted (by me personally) as follows. Lines (0), (2) and (4) contain information on the neural network architecture. In terms of the typical drawings of neural networks using nodes, the features denote the number of nodes. Indeed, the input layer has 784 input nodes. This comes from 28 x 28 = 724, so these nodes represent the pixels of the dataset. The output is towards a hidden layer. All hidden layers have 512 features. I'm not too sure where this comes from. I thought the number of \"hidden features\" may be related to the fact that we represent these as 10D vectors, but that may give $2^{10} = 1024$ features, which is twice as much as here. Some factor $2$ is missing - maybe due to normalization of the vectors, such that only 9 of them are degrees of freedom? Not sure... The final output layer has 10 output features, and these are the conventional labels as before (0 = T-Shirt and so on). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ec3d9",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\n",
    "\n",
    "Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class. We get the prediction probabilities by passing it through an instance of the nn.Softmax module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2eb622",
   "metadata": {},
   "source": [
    "Below, the input X is a random greyscale field: 28 x 28 as before. NOTE: shape is [1, 28, 28]: the first number is related to the number of images in each of the \"batches\" (in this case, only a single image). This first number is called the **minibatch dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d36fcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device = device)\n",
    "\n",
    "# Plot X, like above\n",
    "img = X[0].squeeze()\n",
    "\n",
    "# Plot it\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Call neural network and make prediction\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeb303",
   "metadata": {},
   "source": [
    "### Model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afca53",
   "metadata": {},
   "source": [
    "Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b027ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3,28,28) # 3, for 3 figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6e7c7",
   "metadata": {},
   "source": [
    "#### nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e097721",
   "metadata": {},
   "source": [
    "nn.Flatten: We initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e62f407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134b8463",
   "metadata": {},
   "source": [
    "Note that, above, the first line is used to get the Flatten function, which is part of the neural network class, as a separate function outside of the neural networks. We can skip this step, as we do in part (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c137b",
   "metadata": {},
   "source": [
    "#### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044ce13",
   "metadata": {},
   "source": [
    "The linear layer is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e0a789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features = 28*28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfed02",
   "metadata": {},
   "source": [
    "__Question:__ Why 20 output features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423a7d3",
   "metadata": {},
   "source": [
    "#### nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b12fc8",
   "metadata": {},
   "source": [
    "nn.ReLU: Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use nn.ReLU between our linear layers, but there’s other activations to introduce non-linearity in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "534f1850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2754,  0.1584,  0.6333, -0.4623,  0.4798, -0.3715, -0.1056,  0.5117,\n",
      "         -0.1805,  0.0628,  0.4670,  0.1774, -0.1796,  0.4503,  0.4220, -0.1664,\n",
      "          0.5512, -0.1496, -0.0010, -0.1214],\n",
      "        [ 0.5376,  0.2432,  0.3664, -0.2721,  0.3685, -0.5282, -0.0057,  0.3079,\n",
      "          0.0418,  0.0192,  0.0846, -0.0133, -0.0085,  0.9007,  0.2108,  0.0835,\n",
      "          0.7500,  0.0809,  0.1142, -0.1203],\n",
      "        [ 0.2543,  0.1581,  0.3466, -0.3639,  0.1763, -0.0905,  0.5186,  0.0815,\n",
      "         -0.1698, -0.1660,  0.3123,  0.0789, -0.1554,  0.5723, -0.1543,  0.0654,\n",
      "          0.4454,  0.1179,  0.2233, -0.5287]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2754, 0.1584, 0.6333, 0.0000, 0.4798, 0.0000, 0.0000, 0.5117, 0.0000,\n",
      "         0.0628, 0.4670, 0.1774, 0.0000, 0.4503, 0.4220, 0.0000, 0.5512, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.5376, 0.2432, 0.3664, 0.0000, 0.3685, 0.0000, 0.0000, 0.3079, 0.0418,\n",
      "         0.0192, 0.0846, 0.0000, 0.0000, 0.9007, 0.2108, 0.0835, 0.7500, 0.0809,\n",
      "         0.1142, 0.0000],\n",
      "        [0.2543, 0.1581, 0.3466, 0.0000, 0.1763, 0.0000, 0.5186, 0.0815, 0.0000,\n",
      "         0.0000, 0.3123, 0.0789, 0.0000, 0.5723, 0.0000, 0.0654, 0.4454, 0.1179,\n",
      "         0.2233, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991eaf9",
   "metadata": {},
   "source": [
    "#### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c77e2a",
   "metadata": {},
   "source": [
    "We organize all of the above steps in one network. This is done via nn.Sequential(), which is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ccbc7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f304a",
   "metadata": {},
   "source": [
    "#### nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa28673",
   "metadata": {},
   "source": [
    "The last linear layer of the neural network returns logits (raw values in [-infty, infty]) which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1. The details about the Softmax function can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61e7d443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0819, 0.1054, 0.1366, 0.1138, 0.0962, 0.1157, 0.0932, 0.0663, 0.0893,\n",
      "         0.1016],\n",
      "        [0.0967, 0.1058, 0.1411, 0.1100, 0.0884, 0.1163, 0.0927, 0.0659, 0.0792,\n",
      "         0.1038],\n",
      "        [0.0901, 0.1062, 0.1300, 0.1032, 0.0998, 0.1313, 0.0852, 0.0690, 0.0854,\n",
      "         0.0999]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2706ab",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65abe0a9",
   "metadata": {},
   "source": [
    "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03140c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer 0\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0128,  0.0328, -0.0283,  ..., -0.0321,  0.0332,  0.0034],\n",
      "        [ 0.0293,  0.0036, -0.0224,  ..., -0.0355, -0.0257,  0.0069]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer 1\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0321,  0.0207], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer 2\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0335,  0.0104, -0.0407,  ...,  0.0073, -0.0072, -0.0430],\n",
      "        [ 0.0278,  0.0236, -0.0267,  ...,  0.0174, -0.0145, -0.0006]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer 3\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0379, 0.0205], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer 4\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0142,  0.0183,  0.0384,  ..., -0.0387,  0.0238,  0.0377],\n",
      "        [ 0.0416, -0.0097,  0.0259,  ...,  0.0231,  0.0304,  0.0437]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer 5\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0074, -0.0192], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "layer_counter = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(\"Layer %d\" % layer_counter)\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "    layer_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c8f33",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "* Even-numbered layers in the above neural network correspond to hidden layers, which receive a certain input (tensor of a certain dimension) and have an output (another tensor of a certain, possible different shape). As such, their shape is characterized by two numbers, as can be seen from torch.size. Their shape is [size_out, size_in].\n",
    "* odd-numbered layers in the above neural network correspond to the ReLU stacks. They have a single dimension, equal to the output dimension of the previous layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514147e",
   "metadata": {},
   "source": [
    "## Automatic differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e4003",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function. Suppose we want to make a neural network that adjusts its parameters such that its predicted outcome is a vector containing zeroes. The expected outcome is frequently called `y`. We make predicitions from this simple model by the operation\n",
    "$$\n",
    "z = wx + b\n",
    "$$\n",
    "and try to optimize the parameters and get `z` as close as possible to `y`.\n",
    "\n",
    "The neural network can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a9cf2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are:  tensor([[ 1.1601, -1.1272,  0.0907],\n",
      "        [-0.6276, -0.3242,  2.3798],\n",
      "        [ 0.1758,  0.4598, -0.4319],\n",
      "        [ 0.0734, -0.0180,  0.9507],\n",
      "        [ 0.6712, -0.0732,  0.0809]], requires_grad=True)\n",
      "Bias is:  tensor([ 1.7938,  0.5476, -0.3545], requires_grad=True)\n",
      "Prediction is:  tensor([ 3.2468, -0.5353,  2.7157], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "\n",
    "# Build the neural network\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "print(\"Weights are: \", w)\n",
    "print(\"Bias is: \", b)\n",
    "\n",
    "# Make a prediction\n",
    "z = torch.matmul(x, w) + b\n",
    "print(\"Prediction is: \", z)\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95324e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1752, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d50d9",
   "metadata": {},
   "source": [
    "In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the requires_grad property of those tensors. You can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method. \n",
    "\n",
    "A function that we apply to tensors to construct computational graph is in fact an object of class Function. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor. You can find more information of Function in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68f12daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f68d6bff2e0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f68d6bff100>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ed9c0",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely  under some fixed values of x and y. To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2697f",
   "metadata": {},
   "source": [
    "### Computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e50643f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1752, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[0.3209, 0.1231, 0.3126],\n",
      "        [0.3209, 0.1231, 0.3126],\n",
      "        [0.3209, 0.1231, 0.3126],\n",
      "        [0.3209, 0.1231, 0.3126],\n",
      "        [0.3209, 0.1231, 0.3126]])\n",
      "tensor([0.3209, 0.1231, 0.3126])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ee918",
   "metadata": {},
   "source": [
    "A few comments are in order:\n",
    "* We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "* We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58069a1a",
   "metadata": {},
   "source": [
    "### Disable gradient tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5408f",
   "metadata": {},
   "source": [
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65bcd985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "    print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fe04432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach() # note that we create a copy of z and store it without the gradient requirement\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49452f5d",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "* To mark some parameters in your neural network as frozen parameters. This is a very common scenario for finetuning a pretrained network\n",
    "* To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2375e019",
   "metadata": {},
   "source": [
    "### More on computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfccb26",
   "metadata": {},
   "source": [
    "Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "* run the requested operation to compute a resulting tensor\n",
    "* maintain the operation’s gradient function in the DAG.\n",
    "\n",
    "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
    "* computes the gradients from each .grad_fn,\n",
    "* accumulates them in the respective tensor’s .grad attribute\n",
    "* using the chain rule, propagates all the way to the leaf tensors.\n",
    "\n",
    "An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n",
    "\n",
    "__Optional:__ Further reading about tensor gradients can be read [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215e4a9",
   "metadata": {},
   "source": [
    "## Optimizing model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7c0e7",
   "metadata": {},
   "source": [
    "Now that we have a model and data it’s time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration (called an epoch) the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous section), and optimizes these parameters using gradient descent. For a more detailed walkthrough of this process, [check out this video on backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ccd52",
   "metadata": {},
   "source": [
    "Below, we copy-paste the code from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5a32f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024d6ee",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb614383",
   "metadata": {},
   "source": [
    "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates (read more about hyperparameter tuning)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "1. Number of Epochs - the number times to iterate over the dataset\n",
    "2. Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
    "3. Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52fa1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244089f",
   "metadata": {},
   "source": [
    "### Optimization loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f959c",
   "metadata": {},
   "source": [
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "1. The Train Loop - iterate over the training dataset and try to converge to optimal parameters.\n",
    "2. The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "Let's briefly familiarize ourselves with some of the concepts used in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c92e5b",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917049b",
   "metadata": {},
   "source": [
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\n",
    "\n",
    "We pass our model’s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d59b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ab2ce",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ed717",
   "metadata": {},
   "source": [
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed. In this example we use Stochastic Gradient Descent (SGD). Stochastic Gradient Descent can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). \n",
    "\n",
    "All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
    "\n",
    "We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "febd42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803026a",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "1. Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "2. Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "3. Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79907a9c",
   "metadata": {},
   "source": [
    "#### Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fcaaa3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "#     '''This is the training loop. It requires\n",
    "#         - the dataloader\n",
    "#         - the model\n",
    "#         - the loss function\n",
    "#         - the optimizer object'''\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation happens here:\n",
    "        optimizer.zero_grad() # reset the gradietnts\n",
    "        loss.backward() # backpropagate the error\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "#         '''This is the training loop. It requires\n",
    "#         - the dataloader\n",
    "#         - the model\n",
    "#         - the loss function '''\n",
    "        \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Below, we call \"no grad\", as we are NOT going to update params here\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d999578",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the model’s improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d302ad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310596  [    0/60000]\n",
      "loss: 2.291528  [ 6400/60000]\n",
      "loss: 2.270650  [12800/60000]\n",
      "loss: 2.262007  [19200/60000]\n",
      "loss: 2.258055  [25600/60000]\n",
      "loss: 2.226152  [32000/60000]\n",
      "loss: 2.232859  [38400/60000]\n",
      "loss: 2.205753  [44800/60000]\n",
      "loss: 2.198505  [51200/60000]\n",
      "loss: 2.165927  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 2.159703 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.176373  [    0/60000]\n",
      "loss: 2.164047  [ 6400/60000]\n",
      "loss: 2.105942  [12800/60000]\n",
      "loss: 2.115578  [19200/60000]\n",
      "loss: 2.070878  [25600/60000]\n",
      "loss: 2.015792  [32000/60000]\n",
      "loss: 2.035710  [38400/60000]\n",
      "loss: 1.966682  [44800/60000]\n",
      "loss: 1.965048  [51200/60000]\n",
      "loss: 1.892540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.887255 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.928944  [    0/60000]\n",
      "loss: 1.899168  [ 6400/60000]\n",
      "loss: 1.776495  [12800/60000]\n",
      "loss: 1.811657  [19200/60000]\n",
      "loss: 1.704570  [25600/60000]\n",
      "loss: 1.657769  [32000/60000]\n",
      "loss: 1.673482  [38400/60000]\n",
      "loss: 1.581205  [44800/60000]\n",
      "loss: 1.592672  [51200/60000]\n",
      "loss: 1.498229  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.508028 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.579827  [    0/60000]\n",
      "loss: 1.546316  [ 6400/60000]\n",
      "loss: 1.389712  [12800/60000]\n",
      "loss: 1.465191  [19200/60000]\n",
      "loss: 1.346457  [25600/60000]\n",
      "loss: 1.337193  [32000/60000]\n",
      "loss: 1.355331  [38400/60000]\n",
      "loss: 1.282015  [44800/60000]\n",
      "loss: 1.303198  [51200/60000]\n",
      "loss: 1.224925  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.240704 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.319500  [    0/60000]\n",
      "loss: 1.302969  [ 6400/60000]\n",
      "loss: 1.132722  [12800/60000]\n",
      "loss: 1.244552  [19200/60000]\n",
      "loss: 1.123311  [25600/60000]\n",
      "loss: 1.137175  [32000/60000]\n",
      "loss: 1.166991  [38400/60000]\n",
      "loss: 1.102843  [44800/60000]\n",
      "loss: 1.128699  [51200/60000]\n",
      "loss: 1.070943  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.079847 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.152081  [    0/60000]\n",
      "loss: 1.155748  [ 6400/60000]\n",
      "loss: 0.969694  [12800/60000]\n",
      "loss: 1.109395  [19200/60000]\n",
      "loss: 0.988517  [25600/60000]\n",
      "loss: 1.005966  [32000/60000]\n",
      "loss: 1.051994  [38400/60000]\n",
      "loss: 0.991978  [44800/60000]\n",
      "loss: 1.016360  [51200/60000]\n",
      "loss: 0.976061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.977376 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.037365  [    0/60000]\n",
      "loss: 1.062268  [ 6400/60000]\n",
      "loss: 0.859892  [12800/60000]\n",
      "loss: 1.020683  [19200/60000]\n",
      "loss: 0.903807  [25600/60000]\n",
      "loss: 0.914579  [32000/60000]\n",
      "loss: 0.976340  [38400/60000]\n",
      "loss: 0.920951  [44800/60000]\n",
      "loss: 0.938817  [51200/60000]\n",
      "loss: 0.912474  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.907654 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.953142  [    0/60000]\n",
      "loss: 0.997587  [ 6400/60000]\n",
      "loss: 0.781440  [12800/60000]\n",
      "loss: 0.958147  [19200/60000]\n",
      "loss: 0.846879  [25600/60000]\n",
      "loss: 0.847831  [32000/60000]\n",
      "loss: 0.922476  [38400/60000]\n",
      "loss: 0.873453  [44800/60000]\n",
      "loss: 0.883014  [51200/60000]\n",
      "loss: 0.866164  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.857193 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.887830  [    0/60000]\n",
      "loss: 0.948755  [ 6400/60000]\n",
      "loss: 0.722253  [12800/60000]\n",
      "loss: 0.911293  [19200/60000]\n",
      "loss: 0.805492  [25600/60000]\n",
      "loss: 0.797367  [32000/60000]\n",
      "loss: 0.881289  [38400/60000]\n",
      "loss: 0.840156  [44800/60000]\n",
      "loss: 0.841211  [51200/60000]\n",
      "loss: 0.829872  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.818408 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.834964  [    0/60000]\n",
      "loss: 0.908808  [ 6400/60000]\n",
      "loss: 0.675498  [12800/60000]\n",
      "loss: 0.874735  [19200/60000]\n",
      "loss: 0.773328  [25600/60000]\n",
      "loss: 0.758228  [32000/60000]\n",
      "loss: 0.847667  [38400/60000]\n",
      "loss: 0.815303  [44800/60000]\n",
      "loss: 0.808490  [51200/60000]\n",
      "loss: 0.799997  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.787067 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initialize loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train and update:\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f2074",
   "metadata": {},
   "source": [
    "## Saving and loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1d244065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aab00e",
   "metadata": {},
   "source": [
    "PyTorch models store the learned parameters in an internal state dictionary, called state_dict. These can be persisted via the torch.save method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "32bd7669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibeauwouters/miniconda3/envs/igwn-py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/thibeauwouters/miniconda3/envs/igwn-py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/thibeauwouters/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.030115842819213867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 553433881,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30c139be7d049ccb1b23c3bb5e2bde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cab41",
   "metadata": {},
   "source": [
    "To load model weights, you need to create an instance of the same model first, and then load the parameters using load_state_dict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42864494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "model.eval() # THIS STEP IS IMPORTANT!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17070d5",
   "metadata": {},
   "source": [
    "Be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229492a",
   "metadata": {},
   "source": [
    "### Saving and Loading Models with Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21fc4c",
   "metadata": {},
   "source": [
    "When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass model (and not model.state_dict()) to the saving function. Models can be saved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "55477e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14d448",
   "metadata": {},
   "source": [
    "And models can be loaded again as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f961cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb0e31",
   "metadata": {},
   "source": [
    "This approach uses Python pickle module when serializing the model, thus it relies on the actual class definition to be available when loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a76e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
